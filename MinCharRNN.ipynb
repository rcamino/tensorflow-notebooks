{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal character RNN\n",
    "\n",
    "![Character sequence](images/charseq.jpeg)\n",
    "\n",
    "Related paper by Andrej Karpathy: [Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. \"Visualizing and understanding recurrent networks.\" arXiv preprint arXiv:1506.02078 (2015).](https://arxiv.org/abs/1506.02078)\n",
    "\n",
    "Related blogpost by Andrej Karpathy: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "Original code by Andrej Karpathy: [gist](https://gist.github.com/karpathy/d4dee566867f8291f086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "A Shakespeare sample can be downloaded from [here](https://github.com/karpathy/char-rnn/raw/master/data/tinyshakespeare/input.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/tinyshakespeare.txt\", \"r\") as data_file:\n",
    "    data = data_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the amount of characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size = len(data)\n",
    "print(data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an alphabet from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = set(data)\n",
    "alphabet_size = len(alphabet)\n",
    "print(alphabet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a number to every symbol in the alphabet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_to_id = {}\n",
    "id_to_symbol = {}\n",
    "for symbol_id, symbol in enumerate(sorted(alphabet)):\n",
    "    symbol_to_id[symbol] = symbol_id\n",
    "    id_to_symbol[symbol_id] = symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a sequence of symbols to a sequence of one hot encoding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_one_hot_encoding(symbols):\n",
    "    one_hot_encoded = np.zeros((len(symbols), alphabet_size))\n",
    "    for row, symbol in enumerate(symbols):\n",
    "        symbol_id = symbol_to_id[symbol]\n",
    "        one_hot_encoded[row, symbol_id] = 1\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "hidden_size = 100\n",
    "sequence_length = 25\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "t_inputs = tf.placeholder(shape=[None, alphabet_size], dtype=tf.float32, name=\"inputs\")\n",
    "t_labels = tf.placeholder(shape=[None, alphabet_size], dtype=tf.float32, name=\"labels\")\n",
    "t_initial_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"state\")\n",
    "\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    t_input_to_hidden = tf.get_variable(\n",
    "        \"input_to_hidden\", [alphabet_size, hidden_size], initializer=initializer)\n",
    "\n",
    "    t_hidden_to_hidden = tf.get_variable(\n",
    "        \"hidden_to_hidden\", [hidden_size, hidden_size], initializer=initializer)\n",
    "\n",
    "    t_hidden_to_output = tf.get_variable(\n",
    "        \"hidden_to_output\", [hidden_size, alphabet_size], initializer=initializer)\n",
    "\n",
    "    t_bias_hidden  = tf.get_variable(\"bias_hidden\", [hidden_size], initializer=initializer)\n",
    "    t_bias_output  = tf.get_variable(\"bias_output\", [alphabet_size], initializer=initializer)\n",
    "        \n",
    "    t_hidden_state = t_initial_state\n",
    "    logits = []\n",
    "    for step, t_step_inputs in enumerate(tf.split(t_inputs, sequence_length, axis=0)):\n",
    "        with tf.variable_scope(\"step\" + str(step)):\n",
    "            t_hidden_state = tf.tanh(\n",
    "                tf.add(\n",
    "                    tf.add(\n",
    "                        tf.matmul(t_step_inputs, t_input_to_hidden),\n",
    "                        tf.matmul(t_hidden_state, t_hidden_to_hidden)\n",
    "                    ),\n",
    "                    t_bias_hidden\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            t_step_logits = tf.matmul(t_hidden_state, t_hidden_to_output) + t_bias_output\n",
    "        logits.append(t_step_logits)\n",
    "        \n",
    "    with tf.variable_scope(\"sampler\"):\n",
    "        t_sampler_hidden_state = tf.tanh(\n",
    "            tf.add(\n",
    "                tf.add(\n",
    "                    tf.matmul(t_inputs, t_input_to_hidden),\n",
    "                    tf.matmul(t_initial_state, t_hidden_to_hidden)\n",
    "                ),\n",
    "                t_bias_hidden\n",
    "            )\n",
    "        )\n",
    "\n",
    "        t_sampler_output = tf.nn.softmax(tf.matmul(t_sampler_hidden_state, t_hidden_to_output) + t_bias_output)\n",
    "\n",
    "t_logits = tf.concat(logits, axis=0)\n",
    "t_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=t_labels, logits=t_logits))\n",
    "\n",
    "t_optimizer = tf.train.AdamOptimizer()\n",
    "gradients = t_optimizer.compute_gradients(t_loss)\n",
    "\n",
    "t_gradient_clipping = tf.constant(5.0, name=\"gradient_clipping\")\n",
    "clipped_gradients = []\n",
    "for t_gradient, t_variable in gradients:\n",
    "    t_clipped_gradient = tf.clip_by_value(t_gradient, -t_gradient_clipping, t_gradient_clipping)\n",
    "    clipped_gradients.append((t_clipped_gradient, t_variable))\n",
    "\n",
    "t_updates = t_optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = data_size // (sequence_length + 1)\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "first_symbol = \"\\n\"\n",
    "\n",
    "def print_sample():\n",
    "    batch_inputs = batch_one_hot_encoding([first_symbol])\n",
    "    hidden_state = np.zeros((1, hidden_size))\n",
    "\n",
    "    sample = \"\"\n",
    "    \n",
    "    for sample_id in range(sample_size):\n",
    "        probabilities, hidden_state = sess.run(\n",
    "            [t_sampler_output, t_sampler_hidden_state],\n",
    "            feed_dict={t_inputs: batch_inputs, t_initial_state: hidden_state})\n",
    "        \n",
    "        symbol_id = np.random.choice(range(alphabet_size), p=probabilities.ravel())\n",
    "        symbol = id_to_symbol[symbol_id]\n",
    "        sample += symbol\n",
    "        batch_inputs = batch_one_hot_encoding([symbol])\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(sample)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "log_every = 10000\n",
    "\n",
    "initial_state = np.zeros((1, hidden_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_id in range(epochs):\n",
    "        # reset the state before every epoch\n",
    "        hidden_state = initial_state\n",
    "\n",
    "        accumulated_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        epoch_accumulated_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for batch_id in range(batches):\n",
    "            batch_start = batch_id * sequence_length\n",
    "            batch_end = batch_start + sequence_length + 1\n",
    "            batch_sequence = batch_one_hot_encoding(data[batch_start:batch_end])\n",
    "\n",
    "            batch_inputs = batch_sequence[:-1,:]\n",
    "            batch_labels = batch_sequence[1:,:]\n",
    "\n",
    "            _, batch_loss, hidden_state = sess.run(\n",
    "                [t_updates, t_loss, t_hidden_state],\n",
    "                feed_dict={t_inputs: batch_inputs, t_labels: batch_labels, t_initial_state: hidden_state})\n",
    "            \n",
    "            accumulated_loss += batch_loss\n",
    "            epoch_accumulated_loss += batch_loss\n",
    "\n",
    "            if batch_id % log_every == log_every - 1:\n",
    "                mean_loss = accumulated_loss / float(log_every)\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time - start_time\n",
    "                accumulated_loss = 0.0\n",
    "                start_time = end_time\n",
    "                \n",
    "                print(\"Batch: {:6d} Loss: {:.4f} Time: {:.2f} seconds\".format(batch_id + 1, mean_loss, elapsed_time))\n",
    "                \n",
    "        mean_loss = epoch_accumulated_loss / float(batches)\n",
    "        elapsed_time = time.time() - epoch_start_time\n",
    "\n",
    "        print(\"Epoch: {:6d} Loss: {:.4f} Time: {:.2f} seconds\".format(epoch_id + 1, mean_loss, elapsed_time))\n",
    "                \n",
    "        print_sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
