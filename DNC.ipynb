{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Neural Computer\n",
    "\n",
    "![DNC architecture](https://storage.googleapis.com/deepmind-live-cms/images/dnc_figure1.width-1500_Zfxk87k.png \"DNC architecture\")\n",
    "\n",
    "![DNC model](images/dnc_model.png \"DNC model\")\n",
    "\n",
    "\n",
    "Original paper:\n",
    "[Graves, Alex, et al. \"Hybrid computing using a neural network with dynamic external memory.\" Nature 538.7626 (2016): 471-476.](https://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html)\n",
    "\n",
    "Author blog post:\n",
    "[Differentiable neural computers](https://deepmind.com/blog/differentiable-neural-computers/)\n",
    "\n",
    "Author implementation (Tensorflow + Sonnet):\n",
    "[deepmind/dnc](https://github.com/deepmind/dnc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "$t \\in \\mathbb{N}$: time step\n",
    "\n",
    "$X$: input domain\n",
    "\n",
    "$Y$: output domain\n",
    "\n",
    "$[v_1; \\ldots; v_k]$: concatenation of $k$ vectors\n",
    "\n",
    "$\\circ$: elementwise multiplication\n",
    "\n",
    "$\\mathcal{S}_K$: $(K-1)$-dimensional unit simplex;\n",
    "$\\mathcal{S}_K = \\{\\alpha \\in \\mathbb{R}^K: \\alpha_i \\in [0, 1], \\sum_{i=1}^K \\alpha_i = 1\\}$\n",
    "\n",
    "$\\Delta_K$: non-negative orthant of $\\mathbb{R}^K$ with $\\mathcal{S}_K$ as boundary;\n",
    "$\\Delta_K = \\{\\alpha \\in \\mathbb{R}^K: \\alpha_i \\in [0, 1], \\sum_{i=1}^K \\alpha_i \\leq 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations\n",
    "\n",
    "* $\\sigma(x) = (1 + e^{-x})^{-1}$\n",
    "* $oneplus(x) = 1 + \\log(1 + e^x)$\n",
    "* $softmax(x)[i] = e^{x[i]} (\\sum_{j=1}^n e^{x[j]})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "$N \\in \\mathbb{N}$: number of memory rows or locations\n",
    "\n",
    "$W \\in \\mathbb{N}$: number of memory columns or word length\n",
    "\n",
    "$R \\in \\mathbb{N}$: number of read heads\n",
    "\n",
    "$L \\in \\mathbb{N}$: number of controller network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "$M_t \\in \\mathbb{R}^{N \\times W}$: memory matrix at time $t$\n",
    "\n",
    "$x_t \\in \\mathbb{R}^X$: input vector at time $t$\n",
    "\n",
    "$z_t \\in \\mathbb{R}^Y$: target vector at time $t$\n",
    "\n",
    "$r_t^i \\in \\mathbb{R}^W$: read vector at time $t$ from read head $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controller network\n",
    "\n",
    "$\\mathcal{N}$: controller network\n",
    "\n",
    "$\\mathcal{X}_t \\in \\mathbb{R}^{X + RW}$: controller input vector at time $t$; $\\mathcal{X}_t = [x_t; r_{t-1}^1; \\ldots, r_{t-1}^R]$\n",
    "\n",
    "$\\theta$: controller parameter vector (all the weights)\n",
    "\n",
    "* Recurrent Neural Network: $(\\nu_t, \\xi_t) = \\mathcal{N}([\\mathcal{X}_1; \\ldots; \\mathcal{X}_t]; \\theta)$\n",
    "\n",
    "* Feed-forward Neural Network: $(\\nu_t, \\xi_t) = \\mathcal{N}(\\mathcal{X}_t; \\theta)$\n",
    "\n",
    "Deep LSTM variant used in the paper:\n",
    "\n",
    "$$i_t^l = \\sigma(W_i^l[\\mathcal{X}_t; h_{t - 1}^l; h_t^{l-1}] + b_i^l)$$\n",
    "\n",
    "$$f_t^l = \\sigma(W_f^l[\\mathcal{X}_t; h_{t - 1}^l; h_t^{l-1}] + b_f^l)$$\n",
    "\n",
    "$$s_t^l = f_t^l s_{t-1}^l + i_t^l tanh(W_s^l[\\mathcal{X}_t; h_{t - 1}^l; h_t^{l-1}] + b_s^l)$$\n",
    "\n",
    "$$o_t^l = \\sigma(W_o^l[\\mathcal{X}_t; h_{t - 1}^l; h_t^{l-1}] + b_o^l)$$\n",
    "\n",
    "$$h_t^l = o_t^l tanh(s_t^l)$$\n",
    "\n",
    "Where for layer $l$ at time $t$:\n",
    "\n",
    "$h_t^l$: hidden state\n",
    "\n",
    "$i_t^l$: input gate\n",
    "\n",
    "$f_t^l$: forget gate\n",
    "\n",
    "$s_t^l$: state gate\n",
    "\n",
    "$o_t^l$: output gate\n",
    "\n",
    "$h_t^0 = 0$, $h_0^l = 0$ and $s_0^l = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Weights / Parameters\n",
    "\n",
    "$W_{\\xi}$: hidden to interface weight matrix\n",
    "\n",
    "$W_y$: hidden to output weight matrix\n",
    "\n",
    "$W_r \\in \\mathbb{R}^{RW \\times Y}$: read vectors to output weight matrix\n",
    "\n",
    "(also the hidden weights of the chosen architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "$y_t \\in \\mathbb{R}^Y$: output vector at time $t$;\n",
    "$y_t = W_y[h_t^1; \\ldots, h_t^L] + W_r[r_t^1; \\ldots, r_t^R]$\n",
    "\n",
    "$\\xi_t \\in \\mathbb{R}^{(W \\times R) + 3W + 5R + 3}$:\n",
    "interface vector (interactions with the memory) at time $t$;\n",
    "$\\xi = W_{\\xi}[h_t^1; \\ldots, h_t^L]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface parameters\n",
    "\n",
    "$\\xi_t= [\n",
    "k_t^{r,1}; \\ldots; k_t^{r,R};\n",
    "\\hat{\\beta}_t^{r,1}; \\ldots; \\hat{\\beta}_t^{r,R};\n",
    "k_t^w; \\hat{\\beta}_t^w; \\hat{e}_t; v_t;\n",
    "\\hat{f}_t^1; \\ldots; \\hat{f}_t^R;\n",
    "\\hat{g}_t^a; \\hat{g}_t^w;\n",
    "\\hat{\\pi}_t^1; \\ldots; \\hat{\\pi}_t^R\n",
    "]$\n",
    "\n",
    "$\\{k_t^{r,i} \\in \\mathbb{R}^W; 1 \\leq i \\leq R\\}$: read keys\n",
    "\n",
    "$\\{\\beta_t^{r,i} = oneplus(\\hat{\\beta}_t^{r,i}) \\in [1, \\infty); 1 \\leq i \\leq R\\}$: read strengths\n",
    "\n",
    "$k_t^w \\in \\mathbb{R}^W$: write key\n",
    "\n",
    "$\\beta_t^w = oneplus(\\hat{\\beta}_t^w) \\in [1, \\infty)$: write strength\n",
    "\n",
    "$e_t = \\sigma(\\hat{e}_t) \\in [0, 1]^W$: erase vector\n",
    "\n",
    "$v_t \\in \\mathbb{R}^W$: write vector\n",
    "\n",
    "$\\{f_t^i = \\sigma(\\hat{f}_t^i) \\in [0, 1]; 1 \\leq i \\leq R\\}$: free gates\n",
    "\n",
    "$g_t^a = \\sigma(\\hat{g}_t^a) \\in [0, 1]$: allocation gate\n",
    "\n",
    "$g_t^w = \\sigma(\\hat{g}_t^w) \\in [0, 1]$: write gate\n",
    "\n",
    "$\\{\\pi_t^i = softmax(\\hat{\\pi}_t^i) \\in \\mathcal{S}_3; 1 \\leq i \\leq R\\}$: read modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read modes / Memory addressing / Content attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity meassure (cossine similarity)\n",
    "\n",
    "Used for reading and writing and it is related to assosiative structures.\n",
    "\n",
    "$$\\mathcal{C}(M, k, \\beta)[i] = \\frac{\\exp\\{\\mathcal{D(k, M[i, \\cdot])\\beta}\\}}\n",
    "{(\\sum_{j=i}^N \\exp\\{\\mathcal{D(k, M[j, \\cdot])\\beta}\\})}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\mathcal{C}(M, k, \\beta) \\in \\mathcal{S}_N$ defines a normalized probability distribution over the memory locations,\n",
    "\n",
    "$k \\in \\mathbb{R}^W$ is the lookup key,\n",
    "\n",
    "$\\beta \\in [1, \\infty)$ is the key strength,\n",
    "\n",
    "and $\\mathcal{D}$ is the cosine similarity defined as:\n",
    "\n",
    "$$\\mathcal{D}(u, v) = \\frac{u \\cdot v}{|u||v|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage vector and memory allocation\n",
    "\n",
    "$\\{f_t^i, 1 \\leq i \\leq R\\}$: free gates; determine whether the most recently read locations can be freed.\n",
    "\n",
    "$\\psi_t \\in [0, 1]^N$: retention vector; represents by how much each location will not be freed by the free gates;\n",
    "$\\psi_t = \\Pi_{i=1}^R (1- f_t^i w_{t-1}^{r, i})$\n",
    "\n",
    "$u_t \\in [0, 1]^N$: usage vector; $u_t = (u_{t-1} + w_{t-1}^w - u_{t-1} \\circ w_{t-1}^w) \\circ \\psi_t$ where $u_0 = 0$\n",
    "\n",
    "A location $i$ is used if it has been retained by the free gates ($\\psi_t[i] \\approx 1$), and were either already in use or have just been written to.\n",
    "\n",
    "$\\phi_t \\in \\mathbb{Z}^N$: free list; is defined by sorting the indices of the memory locations in ascending order of $u_t$\n",
    "\n",
    "$a_t \\in \\Delta_N$: allocation weighting; provides new locations for writing:\n",
    "\n",
    "$$a_t[\\phi_t[j]] = (1 - u_t[\\phi_t[j]]) \\Pi_{i=1}^{j-1} u_t[\\phi_t[i]]$$\n",
    "\n",
    "If all usages are 1, then a $a_t = 0$ and the controller can no longer allocate memory without first freeing used locations.\n",
    "\n",
    "The sort operation induces discontinuities at the points at which the sort order changes. The authors ignore these discontinuities when calculating the gradient, as they do not seem to be relevant to learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal link matrix\n",
    "\n",
    "$p_t \\in \\Delta_N$: precedence weighting;\n",
    "$p_t = \\left(1 - \\sum_i w_t^w[i]\\right) p_{t - 1} + w_t^w$ where $p_0 = 0$\n",
    "\n",
    "$p_t[i]$ represents the degree to which location $i$ was the last one written to.\n",
    "\n",
    "$L \\in [0, 1]^{N \\times N}$: temporal link matrix; where:\n",
    "\n",
    "$$L_0[i, j] = 0, 1 \\leq i, j \\leq N$$\n",
    "\n",
    "$$L_t[i, i] = 0, 1 \\leq i \\leq N$$\n",
    "\n",
    "$$L_t[i, j] = (1 - w_t^w[i] - w_t^w[j]) L_{t - 1}[i, j] + w_t^w[i] p_{t - 1}[j]$$\n",
    "\n",
    "$L[i, \\cdot] \\in \\Delta_N$ and $L[\\cdot, j] \\in \\Delta_N$ for all $1 \\leq i, j \\leq N$.\n",
    "\n",
    "If $L[i, j] \\approx 1$ then $i$ was written after $j$, otherwise $L[i, j] \\approx 0$.\n",
    "\n",
    "$Lw$ smoothly shifts the focus forwards to the locations written after those emphasized in $w$,\n",
    "whereas $L^{\\top}w$ shifts the focus backwards.\n",
    "\n",
    "There is a sparse version of this matrix in the parper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and write weightings\n",
    "\n",
    "### Calculate next state of memory (erase and write)\n",
    "\n",
    "$c_t^w \\in \\mathcal{S}_N$: write content weighting;\n",
    "$c_t^w = \\mathcal{C}(M_{t - 1}, k_t^w, \\beta_t^w)$\n",
    "\n",
    "$w_t^w \\in \\Delta_N$: write weighting;\n",
    "$w_t^w = g_t^w[g_t^a a_t + (1 - g_t^a) c_t^w]$\n",
    "\n",
    "$M_t = M_{t - 1} \\circ (E - w_t^w e_t^{\\top}) + w_t^w v_t^{\\top}$\n",
    "\n",
    "Where $E$ is an $N \\times W$ matrix of ones.\n",
    "\n",
    "### Calculate read vectors\n",
    "\n",
    "For each read head $i$ with $1 \\leq i \\leq R$:\n",
    "\n",
    "$c_t^{r, i} \\in \\mathcal{S}_N$: read content weighting;\n",
    "$c_t^{r, i} = \\mathcal{C}(M_{t - 1}, k_t^{r, i}, \\beta_t^{r, i})$\n",
    "\n",
    "$f_t^i \\in \\Delta_N$: forward weighting; $f_t^i = L_t w_{t - 1}^{r, i}$\n",
    "\n",
    "$b_t^i \\in \\Delta_N$: backward weighting; $b_t^i = L_t^{\\top} w_{t - 1}^{r, i}$\n",
    "\n",
    "$w^{r, i} \\in \\Delta_N$: read weighting;\n",
    "$w^{r, i} = \\pi_t^i[1] b_t^i + \\pi_t^i[2] c_t^{r, i} + \\pi_t^i[3] f_t^i$\n",
    "\n",
    "$r_t^i$: read vector; $r_t^i = M_t^{\\top} w_t^{r, i}$\n",
    "\n",
    "If $\\pi_t^i[2]$ dominates the read mode, then the weighting reverts to content lookup using $k_t^{r, i}$.\n",
    "\n",
    "If $\\pi_t^i[3]$ dominates, then the read head iterates through memory locations in the order they were written, ignoring the read key.\n",
    "\n",
    "If $\\pi_t^i[1]$ dominates, then the read head iterates in the reverse order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
