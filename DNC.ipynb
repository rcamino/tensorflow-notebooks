{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Neural Computer\n",
    "\n",
    "![DNC architecture](https://storage.googleapis.com/deepmind-live-cms/images/dnc_figure1.width-1500_Zfxk87k.png \"DNC architecture\")\n",
    "\n",
    "![DNC model](images/dnc_model.png \"DNC model\")\n",
    "\n",
    "\n",
    "Original paper:\n",
    "[Graves, Alex, et al. \"Hybrid computing using a neural network with dynamic external memory.\" Nature 538.7626 (2016): 471-476.](https://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html)\n",
    "\n",
    "Author blog post:\n",
    "[Differentiable neural computers](https://deepmind.com/blog/differentiable-neural-computers/)\n",
    "\n",
    "Author implementation (Tensorflow + Sonnet):\n",
    "[deepmind/dnc](https://github.com/deepmind/dnc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "$t \\in \\mathbb{N}$: time step\n",
    "\n",
    "$\\mathcal{N}$: controller network\n",
    "\n",
    "$X$: input domain\n",
    "\n",
    "$x_t \\in \\mathbb{R}^X$: input vector at time $t$\n",
    "\n",
    "$Y$: output domain\n",
    "\n",
    "$z_t \\in \\mathbb{R}^Y$: target vector at time $t$\n",
    "\n",
    "$N \\in \\mathbb{N}$: number of memory rows or locations\n",
    "\n",
    "$W \\in \\mathbb{N}$: number of memory columns or word length\n",
    "\n",
    "$M_t \\in \\mathbb{R}^{N \\times W}$: memory matrix at time $t$\n",
    "\n",
    "$R \\in \\mathbb{N}$: number of read heads\n",
    "\n",
    "$w^r \\in \\mathbb{R}^{N \\times R}$: read weighting\n",
    "\n",
    "$w^{r, i} \\in \\mathbb{R}^N$: read weighting for read head $i$\n",
    "\n",
    "$r_t^i \\in \\mathbb{R}^W$: read vector at time $t$ from read head $i$\n",
    "\n",
    "$\\mathcal{X}_t$: controller input vector at time $t$; $\\mathcal{X}_t = [x_t; r_{t-1}^1; \\ldots, r_{t-1}^R]$\n",
    "\n",
    "$L \\in \\mathbb{N}$: number of controller network layers\n",
    "\n",
    "$h_t^L$: activation of the last controller network layer at time $t$\n",
    "\n",
    "$W_{\\nu}$: output weight matrix\n",
    "\n",
    "$\\nu_t \\in \\mathbb{R}^Y$: (intermediate) output vector at time $t$; $\\nu_t = W_{\\nu}h_t^L$\n",
    "\n",
    "$W_{\\xi}$: interface weight matrix\n",
    "\n",
    "$\\xi_t \\in \\mathbb{R}^{(W \\times R) + 3W + 5R + 3}$:\n",
    "interface vector (interactions with the memory) at time $t$;\n",
    "$\\xi = W_{\\xi}h_t^L$\n",
    "\n",
    "$\\theta$: controller network parameters\n",
    "\n",
    "$[r_t^1; \\ldots, r_t^R] \\in \\mathbb{R}^{RW}$: concatenation of read vectors at time $t$\n",
    "\n",
    "$W_r \\in \\mathbb{R}^{RW \\times Y}$: read vectors to output weight matrix\n",
    "\n",
    "$y_t \\in \\mathbb{R}^Y$: output vector at time $t$; $y_t = \\nu_t + W_r[r_t^1; \\ldots, r_t^R]$\n",
    "\n",
    "$\\mathcal{S}_N$: $(N-1)$-dimensional unit simplex;\n",
    "$\\mathcal{S}_N = \\{\\alpha \\in \\mathbb{R}^N: \\alpha_i \\in [0, 1], \\sum_{i=1}^N \\alpha_i = 1\\}$\n",
    "\n",
    "$\\circ$: elementwise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations\n",
    "\n",
    "* $\\sigma(x) = (1 + e^{-x})^{-1}$\n",
    "* $oneplus(x) = 1 + \\log(1 + e^x)$\n",
    "* $softmax(x)[i] = e^{x[i]} (\\sum_{j=1}^n e^{x[j]})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller networks\n",
    "\n",
    "* Recurrent Neural Network: $(\\nu_t, \\xi_t) = \\mathcal{N}([\\mathcal{X}_1; \\ldots; \\mathcal{X}_t]; \\theta)$\n",
    "\n",
    "* Feed-forward Neural Network: $(\\nu_t, \\xi_t) = \\mathcal{N}(\\mathcal{X}_t; \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface parameters\n",
    "\n",
    "$\\xi_t= [\n",
    "k_t^{r,1}; \\ldots; k_t^{r,R};\n",
    "\\hat{\\beta}_t^{r,1}; \\ldots; \\hat{\\beta}_t^{r,R};\n",
    "k_t^w; \\hat{\\beta}_t^w; \\hat{e}_t; v_t;\n",
    "\\hat{f}_t^1; \\ldots; \\hat{f}_t^R;\n",
    "\\hat{g}_t^a; \\hat{g}_t^w;\n",
    "\\hat{\\pi}_t^1; \\ldots; \\hat{\\pi}_t^R\n",
    "]$\n",
    "\n",
    "$\\{k_t^{r,i} \\in \\mathbb{R}^W; 1 \\leq i \\leq R\\}$: read keys\n",
    "\n",
    "$\\{\\beta_t^{r,i} = oneplus(\\hat{\\beta}_t^{r,i}) \\in [1, \\infty); 1 \\leq i \\leq R\\}$: read strengths\n",
    "\n",
    "$k_t^w \\in \\mathbb{R}^W$: write key\n",
    "\n",
    "$\\beta_t^w = oneplus(\\hat{\\beta}_t^w) \\in [1, \\infty)$: write strength\n",
    "\n",
    "$e_t = \\sigma(\\hat{e}_t) \\in [0, 1]^W$: erase vector\n",
    "\n",
    "$v_t \\in \\mathbb{R}^W$: write vector\n",
    "\n",
    "$\\{f_t^i = \\sigma(\\hat{f}_t^i) \\in [0, 1]; 1 \\leq i \\leq R\\}$: free gates\n",
    "\n",
    "$g_t^a = \\sigma(\\hat{g}_t^a) \\in [0, 1]$: allocation gate\n",
    "\n",
    "$g_t^w = \\sigma(\\hat{g}_t^w) \\in [0, 1]$: write gate\n",
    "\n",
    "$\\{\\pi_t^i = softmax(\\hat{\\pi}_t^i) \\in \\mathcal{S}_3; 1 \\leq i \\leq R\\}$: read modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writting\n",
    "\n",
    "$\\Delta_N$: complete set of allowed weightings over $N$ locations;\n",
    "$\\Delta_N = \\{\\alpha \\in \\mathbb{R}^N: \\alpha_i \\in [0, 1], \\sum_{i=1}^N \\alpha_i \\leq 1\\}$\n",
    "\n",
    "### Calculate read vectors\n",
    "\n",
    "$r_t^i = M_t^{\\top} w_t^{r, i}, w_t^{r, i} \\in \\Delta_N, 1 \\leq i \\leq R$\n",
    "\n",
    "They are appended to the input in the next step.\n",
    "\n",
    "### Calculate next state of memory (erase and write)\n",
    "\n",
    "$M_t = M_{t - 1} \\circ (E - w_t^w e_t^{\\top}) + w_t^w v_t^{\\top}$\n",
    "\n",
    "Where $E$ is an $N \\times W$ matrix of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read modes (content attention mechanisms)\n",
    "\n",
    "### Similarity meassure (cossine similarity)\n",
    "\n",
    "Used for reading and writing and it is related to assosiative structures.\n",
    "\n",
    "$$\\mathcal{C}(M, k, \\beta)[i] = \\frac{\\exp\\{\\mathcal{D(k, M[i, \\cdot])\\beta}\\}}\n",
    "{(\\sum_{j=i}^N \\exp\\{\\mathcal{D(k, M[j, \\cdot])\\beta}\\})}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\mathcal{C}(M, k, \\beta) \\in \\mathcal{S}_N$,\n",
    "\n",
    "$k \\in \\mathbb{R}^W$ is the lookup key,\n",
    "\n",
    "$\\beta \\in [1, \\infty)$ is the key strength,\n",
    "\n",
    "and $\\mathcal{D}$ is the cosine similarity defined as:\n",
    "\n",
    "$$\\mathcal{D}(u, v) = \\frac{u \\cdot v}{|u||v|}$$\n",
    "  \n",
    "### Usage vector\n",
    "\n",
    "Used for memory allocation, increased after write and decreased after read.\n",
    "\n",
    "$$u_t = (u_{t-1} + w_{t-1}^w - u_{t-1} \\circ w_{t-1}^w) \\circ \\psi_t  \\in [0, 1]^N$$\n",
    "\n",
    "Where $\\psi_t \\in [0, 1]^N$ represents by how much each location will not be freed by the free gates:\n",
    "\n",
    "$$\\psi_t = \\Pi_{i=1}^R (1- f_t^i w_{t-1}^{r, i})$$\n",
    "\n",
    "A location $i$ is used if it has been retained by the free gates ($\\psi_t[i] \\approx 1$), and were either already in use or have just been written to.\n",
    "\n",
    "### Temporal link matrix\n",
    "\n",
    "Used for secuential retrieval.\n",
    "\n",
    "$$L \\in [0, 1]^{N \\times N}$$\n",
    "\n",
    "If $L[i, j] \\approx 1$ then $i$ was written after $j$, otherwise $L[i, j] \\approx 0$.\n",
    "\n",
    "$Lw$ smoothly shifts the focus forwards to the locations written after those emphasized in $w$,\n",
    "whereas $L^{\\top}w$ shifts the focus backwards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
