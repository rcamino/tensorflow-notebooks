{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Neural Computer\n",
    "\n",
    "![DNC architecture](https://storage.googleapis.com/deepmind-live-cms/images/dnc_figure1.width-1500_Zfxk87k.png \"DNC architecture\")\n",
    "\n",
    "![DNC model](images/dnc_model.png \"DNC model\")\n",
    "\n",
    "\n",
    "Paper from Deep Mind:\n",
    "[Graves, Alex, et al. \"Hybrid computing using a neural network with dynamic external memory.\" Nature 538.7626 (2016): 471-476.](https://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html)\n",
    "\n",
    "Deep Mind blog post:\n",
    "[Differentiable neural computers](https://deepmind.com/blog/differentiable-neural-computers/)\n",
    "\n",
    "Deep Mind implementation (Tensorflow + Sonnet):\n",
    "[deepmind/dnc](https://github.com/deepmind/dnc)\n",
    "\n",
    "This code was based in Siraj Raval [tutorial](https://www.youtube.com/watch?v=r5XKzjTFCZQ) and [code](https://github.com/llSourcell/differentiable_neural_computer_LIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Every time step dependant operation will be defined inside a function, and shared weights and constants will be defined outside. All functions will receive a dictionary `tm1` ($t - 1$) with all the values from the previous step and another dictionary `t` with part of the information of the current step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "tf.reset_default_graph() # for executing this block more than once\n",
    "\n",
    "epsilon = 1e-6\n",
    "normal_stddev = 0.1\n",
    "t_normal_initializer = tf.truncated_normal_initializer(mean=0, stddev=normal_stddev) # initialization ~ N(0, 0.1)\n",
    "t_epsilon_initializer = tf.constant_initializer(value=epsilon) # initialization with small positive constant\n",
    "t_zeros_initializer = tf.zeros_initializer() # initialization with zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "$t \\in \\mathbb{N}$: time step\n",
    "\n",
    "$X$: input domain\n",
    "\n",
    "$Y$: output domain\n",
    "\n",
    "$M_t \\in \\mathbb{R}^{N \\times W}$: memory matrix at time $t$\n",
    "\n",
    "$r_t^i \\in \\mathbb{R}^W$: read vector at time $t$ from read head $i$\n",
    "\n",
    "$[v_1; \\ldots; v_k]$: concatenation of $k$ vectors\n",
    "\n",
    "$\\circ$: elementwise multiplication\n",
    "\n",
    "$\\mathcal{S}_K$: $(K-1)$-dimensional unit simplex;\n",
    "$\\mathcal{S}_K = \\{\\alpha \\in \\mathbb{R}^K: \\alpha_i \\in [0, 1], \\sum_{i=1}^K \\alpha_i = 1\\}$\n",
    "\n",
    "$\\Delta_K$: non-negative orthant of $\\mathbb{R}^K$ with $\\mathcal{S}_K$ as boundary;\n",
    "$\\Delta_K = \\{\\alpha \\in \\mathbb{R}^K: \\alpha_i \\in [0, 1], \\sum_{i=1}^K \\alpha_i \\leq 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "These numbers are problem dependant. In this example we will use sequences of 10 one hot encoding vectors of size 4.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 6\n",
    "sequence_width = 4\n",
    "\n",
    "input_size = sequence_width # X\n",
    "output_size = sequence_width # Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations\n",
    "\n",
    "* $\\sigma(x) = (1 + e^{-x})^{-1}$\n",
    "* $oneplus(x) = 1 + softplus(x) = 1 + \\log(1 + e^x)$\n",
    "* $softmax(x)[i] = e^{x[i]} (\\sum_{j=1}^n e^{x[j]})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "$N \\in \\mathbb{N}$: number of memory rows or locations\n",
    "\n",
    "$W \\in \\mathbb{N}$: number of memory columns or word length\n",
    "\n",
    "$R \\in \\mathbb{N}$: number of read heads\n",
    "\n",
    "$L \\in \\mathbb{N}$: number of controller network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = 10 # N\n",
    "word_size = 4 # W\n",
    "num_read_heads = 1 # R\n",
    "num_layers = 2 # L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "$x_t \\in \\mathbb{R}^X$: input vector at time $t$\n",
    "\n",
    "$z_t \\in \\mathbb{R}^Y$: target vector at time $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_inputs = tf.placeholder(tf.float32, [sequence_length * 2, input_size], name=\"x\")\n",
    "t_targets = tf.placeholder(tf.float32, [sequence_length * 2, output_size], name=\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller network\n",
    "\n",
    "$\\mathcal{N}$: controller network\n",
    "\n",
    "$\\mathcal{X}_t \\in \\mathbb{R}^{X + RW}$: controller input vector at time $t$; $\\mathcal{X}_t = [x_t; r_{t-1}^1; \\ldots, r_{t-1}^R]$\n",
    "\n",
    "$\\theta$: controller parameter vector (all the weights)\n",
    "\n",
    "* Recurrent Neural Network: $(\\nu_t, \\xi_t) = \\mathcal{N}([\\mathcal{X}_1; \\ldots; \\mathcal{X}_t]; \\theta)$\n",
    "\n",
    "* Feed-forward Neural Network: $(\\nu_t, \\xi_t) = \\mathcal{N}(\\mathcal{X}_t; \\theta)$\n",
    "\n",
    "Deep LSTM variant used in the paper:\n",
    "\n",
    "$$i_t^l = \\sigma(W_i^l[\\mathcal{X}_t; h_{t - 1}^l; h_t^{l-1}] + b_i^l)$$\n",
    "\n",
    "$$f_t^l = \\sigma(W_f^l[\\mathcal{X}_t; h_{t - 1}^l; h_t^{l-1}] + b_f^l)$$\n",
    "\n",
    "$$s_t^l = f_t^l s_{t-1}^l + i_t^l tanh(W_s^l[\\mathcal{X}_t; h_{t - 1}^l; h_t^{l-1}] + b_s^l)$$\n",
    "\n",
    "$$o_t^l = \\sigma(W_o^l[\\mathcal{X}_t; h_{t - 1}^l; h_t^{l-1}] + b_o^l)$$\n",
    "\n",
    "$$h_t^l = o_t^l tanh(s_t^l)$$\n",
    "\n",
    "Where for layer $l$ at time $t$:\n",
    "\n",
    "$h_t^l$: hidden state\n",
    "\n",
    "$i_t^l$: input gate\n",
    "\n",
    "$f_t^l$: forget gate\n",
    "\n",
    "$s_t^l$: state gate\n",
    "\n",
    "$o_t^l$: output gate\n",
    "\n",
    "$h_t^0 = 0$, $h_0^l = 0$ and $s_0^l = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "__We will use a simple feed-forward network of two layers__\n",
    "\n",
    "So the value $[h_t^1; \\ldots, h_t^L]$ will be replaced by last activation of the network\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_hidden1_size = 32\n",
    "\n",
    "interface_size = word_size * num_read_heads + 3 * word_size + 5 * num_read_heads + 3\n",
    "controller_input_size = input_size + num_read_heads * word_size\n",
    "controller_output_size = output_size + interface_size\n",
    "\n",
    "with tf.variable_scope(\"N\"):\n",
    "    with tf.variable_scope(\"fc1\"):\n",
    "        t_W1 = tf.get_variable(\n",
    "            \"W\", [controller_input_size, fc_hidden1_size], dtype=np.float32, initializer=t_normal_initializer)\n",
    "\n",
    "        t_b1 = tf.get_variable(\n",
    "            \"b\", [fc_hidden1_size], dtype=np.float32, initializer=t_zeros_initializer)\n",
    "\n",
    "    with tf.variable_scope(\"fc2\"):\n",
    "        t_W2 = tf.get_variable(\n",
    "            \"W\", [fc_hidden1_size, controller_output_size], dtype=np.float32, initializer=t_normal_initializer)\n",
    "\n",
    "        t_b2 = tf.get_variable(\n",
    "            \"b\", [controller_output_size], dtype=np.float32, initializer=t_zeros_initializer)\n",
    "\n",
    "def controller(tm1, t):\n",
    "    with tf.variable_scope(\"N\"):\n",
    "        t[\"controller_input\"] = tf.concat(\n",
    "            [t[\"input\"], tf.reshape(tm1[\"read\"], [1, num_read_heads * word_size])], 1, name=\"X\")\n",
    "\n",
    "        with tf.variable_scope(\"fc1\"):\n",
    "            t_z1 = tf.add(tf.matmul(t[\"controller_input\"], t_W1, name=\"Wa\"), t_b1, name=\"z\")\n",
    "\n",
    "            t_a1 = tf.nn.tanh(t_z1, name=\"a\")\n",
    "\n",
    "        with tf.variable_scope(\"fc2\"):\n",
    "            t_z2 = tf.add(tf.matmul(t_a1, t_W2, name=\"Wa\"), t_b2, name=\"z\")\n",
    "\n",
    "            t_a2 = tf.nn.tanh(t_z2, name=\"a\")\n",
    "            \n",
    "    t[\"controller_activation\"] = t_a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate interface\n",
    "\n",
    "$W_{\\xi}$: hidden to interface weight matrix\n",
    "\n",
    "$\\xi_t \\in \\mathbb{R}^{WR + 3W + 5R + 3}$:\n",
    "interface vector (interactions with the memory) at time $t$;\n",
    "$\\xi = W_{\\xi}[h_t^1; \\ldots, h_t^L]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_hidden_to_interface = tf.get_variable(\n",
    "    \"W_xi\", [controller_output_size, interface_size], dtype=np.float32, initializer=t_normal_initializer)\n",
    "\n",
    "def calculate_interface(tm1, t):\n",
    "    t[\"interface\"] = tf.matmul(t[\"controller_activation\"], t_hidden_to_interface, name=\"xi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the interface into parts\n",
    "\n",
    "$\\xi_t= [\n",
    "k_t^{r,1}; \\ldots; k_t^{r,R};\n",
    "\\hat{\\beta}_t^{r,1}; \\ldots; \\hat{\\beta}_t^{r,R};\n",
    "k_t^w; \\hat{\\beta}_t^w; \\hat{e}_t; v_t;\n",
    "\\hat{f}_t^1; \\ldots; \\hat{f}_t^R;\n",
    "\\hat{g}_t^a; \\hat{g}_t^w;\n",
    "\\hat{\\pi}_t^1; \\ldots; \\hat{\\pi}_t^R\n",
    "]$\n",
    "\n",
    "$\\{k_t^{r,i} \\in \\mathbb{R}^W; 1 \\leq i \\leq R\\}$: read keys\n",
    "\n",
    "$\\{\\beta_t^{r,i} = oneplus(\\hat{\\beta}_t^{r,i}) \\in [1, \\infty); 1 \\leq i \\leq R\\}$: read strengths\n",
    "\n",
    "$k_t^w \\in \\mathbb{R}^W$: write key\n",
    "\n",
    "$\\beta_t^w = oneplus(\\hat{\\beta}_t^w) \\in [1, \\infty)$: write strength\n",
    "\n",
    "$e_t = \\sigma(\\hat{e}_t) \\in [0, 1]^W$: erase vector\n",
    "\n",
    "$v_t \\in \\mathbb{R}^W$: write vector\n",
    "\n",
    "$\\{f_t^i = \\sigma(\\hat{f}_t^i) \\in [0, 1]; 1 \\leq i \\leq R\\}$: free gates\n",
    "\n",
    "$g_t^a = \\sigma(\\hat{g}_t^a) \\in [0, 1]$: allocation gate\n",
    "\n",
    "$g_t^w = \\sigma(\\hat{g}_t^w) \\in [0, 1]$: write gate\n",
    "\n",
    "$\\{\\pi_t^i = softmax(\\hat{\\pi}_t^i) \\in \\mathcal{S}_3; 1 \\leq i \\leq R\\}$: read modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the partition will be of size 10;\n",
    "# this tensor indicates for each position the corresponding part\n",
    "t_partition = tf.constant([\n",
    "    [0] * (num_read_heads * word_size) + \\\n",
    "    [1] * (num_read_heads) + \\\n",
    "    [2] * (word_size) + \\\n",
    "    [3] + \\\n",
    "    [4] * (word_size) + \\\n",
    "    [5] * (word_size) + \\\n",
    "    [6] * (num_read_heads) + \\\n",
    "    [7] + \\\n",
    "    [8] + \\\n",
    "    [9] * (num_read_heads * 3)\n",
    "], dtype=tf.int32)\n",
    "\n",
    "def split_interface(tm1, t):\n",
    "    (t_read_keys,\n",
    "     t_read_strengths,\n",
    "     t_write_key,\n",
    "     t_write_strength,\n",
    "     t_erase,\n",
    "     t_write,\n",
    "     t_free_gates,\n",
    "     t_allocation_gate,\n",
    "     t_write_gate,\n",
    "     t_read_modes) = tf.dynamic_partition(t[\"interface\"], t_partition, 10)\n",
    "\n",
    "    # reshape and apply activations to each part\n",
    "    \n",
    "    with tf.variable_scope(\"k-r\"):\n",
    "        t[\"read_keys\"] = tf.reshape(t_read_keys, [num_read_heads, word_size])\n",
    "\n",
    "    with tf.variable_scope(\"beta-r\"):\n",
    "        t[\"read_strengths\"] = tf.add(1.0, tf.nn.softplus(tf.expand_dims(t_read_strengths, 0)))\n",
    "\n",
    "    with tf.variable_scope(\"k-w\"):\n",
    "        t[\"write_key\"] = tf.expand_dims(t_write_key, 0)\n",
    "\n",
    "    with tf.variable_scope(\"beta-w\"):\n",
    "        t[\"write_strength\"] = tf.add(1.0, tf.nn.softplus(tf.expand_dims(t_write_strength, 0)))\n",
    "\n",
    "    with tf.variable_scope(\"e\"):\n",
    "        t[\"erase\"] = tf.nn.sigmoid(tf.expand_dims(t_erase, 0))\n",
    "\n",
    "    with tf.variable_scope(\"v\"):\n",
    "        t[\"write\"] = tf.expand_dims(t_write, 0)\n",
    "\n",
    "    with tf.variable_scope(\"f\"):\n",
    "        t[\"free_gates\"] = tf.nn.sigmoid(tf.expand_dims(t_free_gates, 0))\n",
    "\n",
    "    t[\"allocation_gate\"] = tf.nn.sigmoid(t_allocation_gate, name=\"g-a\")\n",
    "\n",
    "    t[\"write_gate\"] = tf.nn.sigmoid(t_write_gate, name=\"g-w\")\n",
    "\n",
    "    with tf.variable_scope(\"pi\"):\n",
    "        t[\"read_modes\"] = tf.nn.softmax(tf.reshape(t_read_modes, [3, num_read_heads]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based addressing\n",
    "\n",
    "Used for reading and writing and it is related to assosiative structures.\n",
    "\n",
    "$$\\mathcal{C}(M, k, \\beta)[i] = \\frac{exp\\{\\mathcal{D(k, M[i, \\cdot])\\beta}\\}}\n",
    "{(\\sum_{j=i}^N exp\\{\\mathcal{D(k, M[j, \\cdot])\\beta}\\})}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\mathcal{C}(M, k, \\beta) \\in \\mathcal{S}_N$ defines a normalized probability distribution over the memory locations,\n",
    "\n",
    "$k \\in \\mathbb{R}^W$ is the lookup key,\n",
    "\n",
    "$\\beta \\in [1, \\infty)$ is the key strength,\n",
    "\n",
    "and $\\mathcal{D}$ is the cosine similarity defined as:\n",
    "\n",
    "$$\\mathcal{D}(u, v) = \\frac{u \\cdot v}{|u||v|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is function will be called for read and write\n",
    "# so it is not defined like the rest\n",
    "def content_lookup(t_memory, t_key, t_strength):\n",
    "    with tf.variable_scope(\"C\"):\n",
    "        with tf.variable_scope(\"D\"):\n",
    "            t_normalized_memory = tf.nn.l2_normalize(t_memory, 1, name=\"M_norm\")\n",
    "            t_normalized_key = tf.nn.l2_normalize(t_key, 0, name=\"k_norm\")\n",
    "            t_similarity = tf.matmul(t_normalized_memory, t_normalized_key, transpose_b=True) \n",
    "        return tf.nn.softmax(t_similarity * t_strength, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic memory allocation\n",
    "\n",
    "$\\psi_t \\in [0, 1]^N$: retention vector; represents by how much each location will not be freed by the free gates;\n",
    "$\\psi_t = \\Pi_{i=1}^R (1- f_t^i w_{t-1}^{r, i})$\n",
    "\n",
    "$u_t \\in [0, 1]^N$: usage vector; $u_t = (u_{t-1} + w_{t-1}^w - u_{t-1} \\circ w_{t-1}^w) \\circ \\psi_t$ where $u_0 = 0$\n",
    "\n",
    "A location $i$ is used if it has been retained by the free gates ($\\psi_t[i] \\approx 1$), and were either already in use or have just been written to.\n",
    "\n",
    "$\\phi_t \\in \\mathbb{Z}^N$: free list; is defined by sorting the indices of the memory locations in ascending order of $u_t$\n",
    "\n",
    "$a_t \\in \\Delta_N$: allocation weighting; provides new locations for writing:\n",
    "\n",
    "$$a_t[\\phi_t[j]] = (1 - u_t[\\phi_t[j]]) \\Pi_{i=1}^{j-1} u_t[\\phi_t[i]]$$\n",
    "\n",
    "If all usages are 1, then a $a_t = 0$ and the controller can no longer allocate memory without first freeing used locations.\n",
    "\n",
    "The sort operation induces discontinuities at the points at which the sort order changes. The authors ignore these discontinuities when calculating the gradient, as they do not seem to be relevant to learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def memory_allocation(tm1, t):\n",
    "    with tf.variable_scope(\"psi\"):\n",
    "        t[\"retention\"] = tf.reduce_prod(1 - t[\"free_gates\"] * tm1[\"read_weighting\"], reduction_indices=1)\n",
    "\n",
    "    with tf.variable_scope(\"u\"):\n",
    "        t[\"usage\"] = (tm1[\"usage\"] + tm1[\"write_weighting\"] - tm1[\"usage\"] * tm1[\"write_weighting\"]) * \\\n",
    "            t[\"retention\"]\n",
    "    \n",
    "    with tf.variable_scope(\"phi\"):\n",
    "        # trick to sort in ascending mode:\n",
    "        # negate the usage vector and select the top k positions,\n",
    "        # with k equal to all the positions,\n",
    "        # and then negate back to get the original usage vector with ascending sorting\n",
    "        t_sorted_usage, t_free = tf.nn.top_k(-1 * t[\"usage\"], k=num_words)\n",
    "        t_sorted_usage *= -1\n",
    "\n",
    "        t[\"free\"] = t_free\n",
    "    \n",
    "    with tf.variable_scope(\"a\"):\n",
    "        ## TODO: review this part ##\n",
    "\n",
    "        t_cumprod = tf.cumprod(t_sorted_usage, axis=0, exclusive=True)\n",
    "        t_unorder = (1 - t_sorted_usage) * t_cumprod\n",
    "\n",
    "        t_allocation_weighting = tf.zeros([num_words])\n",
    "\n",
    "        t_identity = tf.constant(np.identity(num_words, dtype=np.float32))\n",
    "\n",
    "        for pos, idx in enumerate(tf.unstack(t_free[0])):\n",
    "            m = tf.squeeze(tf.slice(t_identity, [idx, 0], [1, -1]))\n",
    "            t_allocation_weighting += m * t_unorder[0, pos]\n",
    "\n",
    "        t_allocation_weighting = tf.reshape(t_allocation_weighting, [num_words, 1])\n",
    "\n",
    "        ## -- ##\n",
    "    \n",
    "    t[\"allocation_weighting\"] = t_allocation_weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate next state of memory\n",
    "\n",
    "$c_t^w \\in \\mathcal{S}_N$: write content weighting;\n",
    "$c_t^w = \\mathcal{C}(M_{t - 1}, k_t^w, \\beta_t^w)$\n",
    "\n",
    "$w_t^w \\in \\Delta_N$: write weighting;\n",
    "$w_t^w = g_t^w[g_t^a a_t + (1 - g_t^a) c_t^w]$\n",
    "\n",
    "$M_t = M_{t - 1} \\circ (E - w_t^w e_t^{\\top}) + w_t^w v_t^{\\top}$\n",
    "\n",
    "Where $E$ is an $N \\times W$ matrix of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_memory(tm1, t):\n",
    "    with tf.variable_scope(\"c-w\"):\n",
    "        t[\"write_content_weighting\"] = content_lookup(tm1[\"memory\"], t[\"write_key\"], t[\"write_strength\"])\n",
    "    \n",
    "    with tf.variable_scope(\"w-w\"):\n",
    "        t[\"write_weighting\"] = t[\"write_gate\"] * \\\n",
    "            (t[\"allocation_gate\"] * t[\"allocation_weighting\"] + \\\n",
    "             (1 - t[\"allocation_gate\"]) * t[\"write_content_weighting\"])\n",
    "    \n",
    "    with tf.variable_scope(\"M\"):\n",
    "        t[\"memory\"] = tm1[\"memory\"] * (1 - tf.matmul(t[\"write_weighting\"], t[\"erase\"])) + \\\n",
    "            tf.matmul(t[\"write_weighting\"], t[\"write\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal memory linkage\n",
    "\n",
    "$p_t \\in \\Delta_N$: precedence weighting;\n",
    "$p_t = \\left(1 - \\sum_i w_t^w[i]\\right) p_{t - 1} + w_t^w$ where $p_0 = 0$\n",
    "\n",
    "$p_t[i]$ represents the degree to which location $i$ was the last one written to.\n",
    "\n",
    "$L \\in [0, 1]^{N \\times N}$: temporal link matrix; where:\n",
    "\n",
    "$$L_0[i, j] = 0, 1 \\leq i, j \\leq N$$\n",
    "\n",
    "$$L_t[i, i] = 0, 1 \\leq i \\leq N$$\n",
    "\n",
    "$$L_t[i, j] = (1 - w_t^w[i] - w_t^w[j]) L_{t - 1}[i, j] + w_t^w[i] p_{t - 1}[j]$$\n",
    "\n",
    "$L[i, \\cdot] \\in \\Delta_N$ and $L[\\cdot, j] \\in \\Delta_N$ for all $1 \\leq i, j \\leq N$.\n",
    "\n",
    "If $L[i, j] \\approx 1$ then $i$ was written after $j$, otherwise $L[i, j] \\approx 0$.\n",
    "\n",
    "$Lw$ smoothly shifts the focus forwards to the locations written after those emphasized in $w$,\n",
    "whereas $L^{\\top}w$ shifts the focus backwards.\n",
    "\n",
    "There is a sparse version of this matrix in the parper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def memory_linkage(tm1, t):\n",
    "    with tf.variable_scope(\"L\"):\n",
    "        # trick to vectorize the calculation: repeat the write in N columns\n",
    "        t_write_weighting_repeat = tf.matmul(t[\"write_weighting\"], tf.ones([1, num_words]))\n",
    "\n",
    "        t[\"link\"] = (1 - t_write_weighting_repeat - tf.transpose(t_write_weighting_repeat)) * tm1[\"link\"] + \\\n",
    "            tf.matmul(t[\"write_weighting\"], tm1[\"precedence_weighting\"], transpose_b=True)\n",
    "\n",
    "        # trick to vectorize the calculation:\n",
    "        # multiply by a matrix of ones with zeros in the diagonal\n",
    "        # to turn into zeros the diagonal of the link matrix\n",
    "        t[\"link\"] *= tf.ones([num_words, num_words]) - tf.constant(np.identity(num_words, dtype=np.float32))\n",
    "    \n",
    "    with tf.variable_scope(\"p\"):\n",
    "        t[\"precedence_weighting\"] = (1 - tf.reduce_sum(t[\"write_weighting\"], reduction_indices=0)) * \\\n",
    "            tm1[\"precedence_weighting\"] + t[\"write_weighting\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate read vectors\n",
    "\n",
    "For each read head $i$ with $1 \\leq i \\leq R$:\n",
    "\n",
    "$c_t^{r, i} \\in \\mathcal{S}_N$: read content weighting;\n",
    "$c_t^{r, i} = \\mathcal{C}(M_{t - 1}, k_t^{r, i}, \\beta_t^{r, i})$\n",
    "\n",
    "$f_t^i \\in \\Delta_N$: forward weighting; $f_t^i = L_t w_{t - 1}^{r, i}$ *NOTE: the authors used the same symbol for the free gates!*\n",
    "\n",
    "$b_t^i \\in \\Delta_N$: backward weighting; $b_t^i = L_t^{\\top} w_{t - 1}^{r, i}$\n",
    "\n",
    "$w^{r, i} \\in \\Delta_N$: read weighting;\n",
    "$w^{r, i} = \\pi_t^i[1] b_t^i + \\pi_t^i[2] c_t^{r, i} + \\pi_t^i[3] f_t^i$\n",
    "\n",
    "$r_t^i$: read vector; $r_t^i = M_t^{\\top} w_t^{r, i}$\n",
    "\n",
    "If $\\pi_t^i[2]$ dominates the read mode, then the weighting reverts to content lookup using $k_t^{r, i}$.\n",
    "\n",
    "If $\\pi_t^i[3]$ dominates, then the read head iterates through memory locations in the order they were written, ignoring the read key.\n",
    "\n",
    "If $\\pi_t^i[1]$ dominates, then the read head iterates in the reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_read(tm1, t):\n",
    "    with tf.variable_scope(\"c-r\"):\n",
    "        t[\"read_content_weighting\"] = content_lookup(tm1[\"memory\"], t[\"read_keys\"], t[\"read_strengths\"])\n",
    "    \n",
    "    t[\"forward_weighting\"] = tf.matmul(t[\"link\"], tm1[\"read_weighting\"], name=\"fw\")\n",
    "    t[\"backwards_weighting\"] = tf.matmul(t[\"link\"], tm1[\"read_weighting\"], transpose_a=True, name=\"b\")\n",
    "\n",
    "    with tf.variable_scope(\"w-r\"):\n",
    "        t[\"read_weighting\"] = t[\"read_modes\"][0] * t[\"backwards_weighting\"] + \\\n",
    "            t[\"read_modes\"][1] * t[\"read_content_weighting\"] + \\\n",
    "            t[\"read_modes\"][2] * t[\"forward_weighting\"]\n",
    "        \n",
    "    t[\"read\"] = tf.matmul(t[\"memory\"], t[\"read_weighting\"], transpose_a=True, name=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate output\n",
    "\n",
    "$W_y$: hidden to output weight matrix\n",
    "\n",
    "$W_r \\in \\mathbb{R}^{RW \\times Y}$: read vectors to output weight matrix\n",
    "\n",
    "$\\nu_t \\in \\mathbb{R}^Y$: controller output vector at time $t$;\n",
    "$\\nu_t = W_y[h_t^1; \\ldots, h_t^L]$\n",
    "\n",
    "$y_t \\in \\mathbb{R}^Y$: output vector at time $t$;\n",
    "$y_t = \\nu_t + W_r[r_t^1; \\ldots, r_t^R]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_hidden_to_output = tf.get_variable(\n",
    "    \"W_y\", [controller_output_size, output_size], dtype=np.float32, initializer=t_normal_initializer)\n",
    "\n",
    "t_read_to_output = tf.get_variable(\n",
    "    \"W_r\", [num_read_heads * word_size, output_size], dtype=np.float32, initializer=t_normal_initializer)\n",
    "\n",
    "def calculate_output(tm1, t):\n",
    "    t[\"controller_output\"] = tf.matmul(t[\"controller_activation\"], t_hidden_to_output, name=\"nu\")\n",
    "    \n",
    "    t[\"output\"] = tf.add(\n",
    "        t[\"controller_output\"],\n",
    "        tf.matmul(tf.reshape(t[\"read\"], [1, num_read_heads * word_size]), t_read_to_output),\n",
    "        name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial step\n",
    "\n",
    "According to the paper: $u_0 = 0$ and $p_0 = 0$. For $M_0$, $r_0$, $w_0^w$, $L_0$ and $w_0^r$ it is not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_step():\n",
    "    with tf.variable_scope(\"t0\"):\n",
    "        return {\n",
    "            \"memory\": tf.zeros([num_words, word_size], name=\"M\"),\n",
    "\n",
    "            \"read\": tf.fill([num_read_heads, word_size], epsilon, name=\"r\"),\n",
    "\n",
    "            \"usage\": tf.zeros([num_words, 1], name=\"u\"),\n",
    "\n",
    "            \"write_weighting\": tf.fill([num_words, 1], epsilon, name=\"w-w\"),\n",
    "\n",
    "            \"precedence_weighting\": tf.zeros([num_words, 1], name=\"p\"),\n",
    "\n",
    "            \"link\": tf.zeros([num_words, num_words], name=\"L\"),\n",
    "\n",
    "            \"read_weighting\": tf.fill([num_words, num_read_heads], epsilon, name=\"w-r\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap one step of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dnc_step(tm1, t):\n",
    "    controller(tm1, t)\n",
    "    calculate_interface(tm1, t)\n",
    "    split_interface(tm1, t)\n",
    "    memory_allocation(tm1, t)\n",
    "    write_memory(tm1, t)\n",
    "    memory_linkage(tm1, t)\n",
    "    calculate_read(tm1, t)\n",
    "    calculate_output(tm1, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dnc():\n",
    "    tm1 = initial_step()\n",
    "    outputs = []\n",
    "    for step, t_input in enumerate(tf.unstack(t_inputs, axis=0)):\n",
    "        t_input = tf.expand_dims(t_input, 0)\n",
    "        t = {\"input\": t_input}\n",
    "        with tf.variable_scope(\"t\" + str(step + 1)):\n",
    "            dnc_step(tm1, t)\n",
    "        tm1 = t\n",
    "        outputs.append(t[\"output\"])\n",
    "    return tf.stack(outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_outputs = tf.squeeze(dnc())\n",
    "\n",
    "t_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=t_outputs, labels=t_targets))\n",
    "\n",
    "regularization = 5e-4\n",
    "regularizers = (tf.nn.l2_loss(t_W1) + tf.nn.l2_loss(t_W2) + tf.nn.l2_loss(t_b1) + tf.nn.l2_loss(t_b2))\n",
    "\n",
    "t_loss += regularization * regularizers\n",
    "\n",
    "learning_rate = 0.001\n",
    "t_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(t_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_indices = np.random.randint(0, sequence_width, size=sequence_length)\n",
    "sequence = np.zeros((sequence_length, sequence_width))\n",
    "sequence[np.arange(sequence_length), one_indices] = 1\n",
    "\n",
    "empty_sequence = np.zeros((sequence_length, sequence_width))\n",
    "\n",
    "inputs = np.concatenate((sequence, empty_sequence), axis=0)\n",
    "targets = np.concatenate((empty_sequence, sequence), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "log_every = 100\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        feed_dict = {t_inputs: inputs, t_targets: targets}\n",
    "        \n",
    "        loss, _, predictions = session.run([t_loss, t_optimizer, t_outputs], feed_dict=feed_dict)\n",
    "        \n",
    "        if iteration % log_every == log_every - 1:\n",
    "            print(iteration + 1, loss)\n",
    "            \n",
    "    for i in range(sequence_length):\n",
    "        j = sequence_length + i\n",
    "        print(np.argmax(inputs[i, :]), np.argmax(targets[j, :]), np.argmax(predictions[j, :]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
