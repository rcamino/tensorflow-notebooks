{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet\n",
    "\n",
    "![LeNet](images/lenet.png)\n",
    "\n",
    "Paper: [LeCun, Yann, et al. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE 86.11 (1998): 2278-2324.](http://yann.lecun.com/exdb/publis/psgz/lecun-98.ps.gz)\n",
    "\n",
    "Webpage: [LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/)\n",
    "\n",
    "Tensorflow MNIST tutorials: [beginner](https://www.tensorflow.org/get_started/mnist/beginners) and [advanced](https://www.tensorflow.org/get_started/mnist/pros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Tensorflow has helpers to load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the dataset\n",
    "\n",
    "We can check the size of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = mnist.train.images.shape[1]\n",
    "image_height = int(np.sqrt(image_size))\n",
    "image_width = int(np.sqrt(image_size))\n",
    "image_channels = 1 # black and white images\n",
    "print(\"{:d} = {:d} x {:d} x {:d}\".format(image_size, image_height, image_width, image_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and how many different labels we have (should be one per digit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_size = mnist.train.labels.shape[1]\n",
    "print(labels_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "\n",
    "First we will define some helper functions and then we will call them several times with diferent parameters for different layers in order to build the network. We will use some variable scopes, variable names and operation names for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper, the weights are initialized with random values drawn from a uniform distribution between $-2.4 / F_i$ and $2.4 / F_i$ where $F_i$ is the number of input dimensions (fan-in). In Tensorflow, this initialization can be implemented with `tf.contrib.layers.variance_scaling_initializer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(name, shape):\n",
    "    return tf.get_variable(name, shape, dtype=np.float32,\n",
    "        initializer=tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode=\"FAN_IN\", uniform=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the bias vectors just with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(name, shape):\n",
    "    return tf.get_variable(name, shape, dtype=np.float32, initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D convolution layers with $tanh$ activation according to the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(name, t_input, patch, input_channels, output_channels):\n",
    "    with tf.variable_scope(name):\n",
    "        t_weight = weight_variable(\"weight\", [patch, patch, input_channels, output_channels])\n",
    "        t_conv = tf.nn.conv2d(t_input, t_weight, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "        t_bias = bias_variable(\"bias\", [output_channels])\n",
    "        t_linear = tf.add(t_conv, t_bias, name=\"linear\")\n",
    "        t_activation = tf.nn.tanh(t_linear, name=\"activation\")\n",
    "    return t_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling layers of 2x2 patches using max pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(name, x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layers with optional $tanh$ activation (last layer uses softmax outside):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(name, t_input, input_size, hidden_size, activation=True):\n",
    "    with tf.variable_scope(name):\n",
    "        t_weight = weight_variable(\"weight\", [input_size, hidden_size])\n",
    "        t_matmul = tf.matmul(t_input, t_weight, name=\"matmul\")\n",
    "        t_bias = bias_variable(\"bias\", [hidden_size])\n",
    "        t_linear = tf.add(t_matmul, t_bias, name=\"linear\")\n",
    "        if activation:\n",
    "            return tf.nn.tanh(t_linear, name=\"activation\")\n",
    "        else:\n",
    "            return t_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this in case you want to build the network again (you will get errors otherwise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate the layers according to the paper diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1_channels = 6\n",
    "conv1_patch = 5\n",
    "conv2_channels = 16\n",
    "conv2_patch = 5\n",
    "fc_input = conv2_channels * conv2_patch * conv2_patch\n",
    "fc1_hidden = 120\n",
    "fc2_hidden = 84\n",
    "\n",
    "t_flat_images = tf.placeholder(tf.float32, shape=[None, image_size], name=\"flat_images\")\n",
    "t_labels = tf.placeholder(tf.float32, shape=[None, labels_size], name=\"labels\")\n",
    "\n",
    "t_square_images = tf.reshape(t_flat_images, [-1, image_height, image_width, image_channels], name=\"square_images\")\n",
    "t_padded_images = tf.pad(t_square_images, [[0, 0], [2, 2], [2, 2], [0, 0]], name=\"padded_images\")\n",
    "\n",
    "t_conv1 = conv2d(\"conv1\", t_padded_images, conv1_patch, image_channels, conv1_channels)\n",
    "\n",
    "t_sampling1 = max_pool_2x2(\"sampling1\", t_conv1)\n",
    "\n",
    "t_conv2 = conv2d(\"conv2\", t_sampling1, conv2_patch, conv1_channels, conv2_channels)\n",
    "\n",
    "t_sampling2 = max_pool_2x2(\"sampling2\", t_conv2)\n",
    "\n",
    "t_fc_input = tf.reshape(t_sampling2, [-1, fc_input], name=\"fc_input\")\n",
    "\n",
    "t_fc1 = fc(\"fc1\", t_fc_input, fc_input, fc1_hidden)\n",
    "\n",
    "t_fc2 = fc(\"fc2\", t_fc1, fc1_hidden, fc2_hidden)\n",
    "\n",
    "t_logits = fc(\"fc3\", t_fc2, fc2_hidden, labels_size, activation=False)\n",
    "t_predictions = tf.nn.softmax(t_logits, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the loss with cross entropy, comparing the digit with the highest predicted probability compared with the correct one and we optimize with gradient descent. Also we calculate the amount of correct predictions to compute the accuracy later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"loss\"):\n",
    "    t_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=t_logits, labels=t_labels))\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.variable_scope(\"optimizer\"):\n",
    "    t_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(t_loss)\n",
    "\n",
    "with tf.variable_scope(\"correct\"):\n",
    "    t_correct = tf.reduce_sum(\n",
    "        tf.cast(\n",
    "            tf.equal(tf.argmax(t_predictions, 1), tf.argmax(t_labels, 1)),\n",
    "            tf.float32\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will update the weights in batches with the gradient descent optimizer we defined before. In test mode, we will only collect loss and accuracy metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "log_every_batches = 100\n",
    "\n",
    "def run_batches(session, batches, train=True):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0.0\n",
    "    \n",
    "    iterations = batches.num_examples // batch_size\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        batch = batches.next_batch(batch_size)\n",
    "\n",
    "        if train:\n",
    "            session.run(t_optimizer, feed_dict={t_flat_images: batch[0], t_labels: batch[1]})\n",
    "            \n",
    "        loss, correct = session.run([t_loss, t_correct],\n",
    "                                    feed_dict={t_flat_images: batch[0], t_labels: batch[1]})\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_correct += correct\n",
    "        \n",
    "        if train and iteration % log_every_batches == log_every_batches - 1:\n",
    "            print(\"Train Batch: {:5d} Loss: {:.4f}\".format(iteration + 1, loss))\n",
    "\n",
    "    return epoch_loss / iterations, epoch_correct / (iterations * batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the training session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_id in range(epochs):\n",
    "        train_loss, train_accuracy = run_batches(session, mnist.train, train=True)\n",
    "        test_loss, test_accuracy = run_batches(session, mnist.test, train=False)\n",
    "\n",
    "        print(\"Epoch: {:5d} Train Loss: {:.4f} Test Loss: {:.4f} Train Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(\n",
    "            epoch_id + 1, train_loss, test_loss, train_accuracy, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
